# JPXå…¨éŠ˜æŸ„5å¹´åˆ†æ ªä¾¡ãƒ‡ãƒ¼ã‚¿åé›†ã‚·ã‚¹ãƒ†ãƒ  - é…å½“æ—¥ãƒ»å‹•çš„æ™‚ä¾¡ç·é¡ãƒ»å¸‚å ´åŒºåˆ†å¯¾å¿œç‰ˆ

import yfinance as yf
import pandas as pd
import numpy as np
import requests
import io
import time
import os
import json
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from tqdm import tqdm
from datetime import datetime, timedelta
import warnings
import logging
from pathlib import Path
import boto3

warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.ERROR)
logger = logging.getLogger(__name__)

class JPXStockCollector:
    """JPXæ ªä¾¡ãƒ‡ãƒ¼ã‚¿åé›† - ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–ãƒ»é…å½“æ—¥ãƒ»å‹•çš„æ™‚ä¾¡ç·é¡å¯¾å¿œ"""

    def __init__(self):
        self.jpx_symbols = []
        self.stock_data = {}
        self.failed_symbols = []
        self.symbol_metadata = {}  # ğŸ†• JPXãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜ç”¨
        self.lock = threading.Lock()
        self.s3_client = None

        self.data_dir = Path("jpx_stock_data")
        self.data_dir.mkdir(exist_ok=True)

        self.config = {
            "max_workers": 3,
            "request_delay": 2.0,
            "chunk_size": 300,
            "chunk_delay": 120
        }

        print("JPXæ ªä¾¡åé›†ã‚·ã‚¹ãƒ†ãƒ  - é…å½“æ—¥ãƒ»å‹•çš„æ™‚ä¾¡ç·é¡å¯¾å¿œç‰ˆ")
        print(f"è¨­å®š: ä¸¦åˆ—{self.config['max_workers']}, å¾…æ©Ÿ{self.config['request_delay']}ç§’, ãƒãƒ£ãƒ³ã‚¯{self.config['chunk_size']}")

    def setup_s3(self):
        """S3æ¥ç¶š"""
        try:
            self.s3_client = boto3.client(
                's3',
                aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),
                aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),
                region_name="ap-northeast-1"
            )
            self.s3_client.head_bucket(Bucket="m-s3storage")
            print("S3æ¥ç¶šæˆåŠŸ: s3://m-s3storage/japan-stocks-5years-chart/")
            return True
        except Exception as e:
            print(f"S3æ¥ç¶šå¤±æ•—: {e}")
            return False

    def get_jpx_symbols(self):
        """JPXéŠ˜æŸ„å–å¾— - Excelãƒ‡ãƒ¼ã‚¿ã®ã¿ä½¿ç”¨"""
        print("JPXéŠ˜æŸ„ãƒªã‚¹ãƒˆå–å¾—ä¸­...")

        jpx_data = self._download_jpx_data()
        if jpx_data is None:
            raise Exception("JPX Excelãƒ•ã‚¡ã‚¤ãƒ«å–å¾—å¤±æ•—")

        print(f"Excelå–å¾—æˆåŠŸ: {len(jpx_data)} è¡Œ")

        symbols = self._extract_symbols(jpx_data)
        if not symbols or len(symbols) < 100:
            raise Exception(f"éŠ˜æŸ„æŠ½å‡ºå¤±æ•—: {len(symbols) if symbols else 0} éŠ˜æŸ„ã®ã¿æŠ½å‡º")

        print(f"JPXå…¬å¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ {len(symbols)} éŠ˜æŸ„æŠ½å‡ºæˆåŠŸ")
        return symbols

    def _download_jpx_data(self):
        """JPX data_j.xlså–å¾—"""
        url = "https://www.jpx.co.jp/markets/statistics-equities/misc/tvdivq0000001vg2-att/data_j.xls"

        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'Referer': 'https://www.jpx.co.jp/markets/statistics-equities/misc/01.html'
            }

            response = requests.get(url, headers=headers, timeout=60)

            if response.status_code == 200 and len(response.content) > 100000:
                content = response.content

                if b'<html' not in content[:500].lower():
                    try:
                        df = pd.read_excel(io.BytesIO(content), engine='xlrd', dtype=str)
                        if len(df) > 1000:
                            print(f"JPX Excelè§£ææˆåŠŸ: {len(df)} è¡Œ")
                            return df
                    except:
                        try:
                            df = pd.read_excel(io.BytesIO(content), engine='calamine', dtype=str)
                            if len(df) > 1000:
                                print(f"JPX Excelè§£ææˆåŠŸ: {len(df)} è¡Œ")
                                return df
                        except:
                            pass

            return None

        except:
            return None

    def _extract_symbols(self, df):
        """éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰æŠ½å‡º - è‹±å­—éŠ˜æŸ„å¯¾å¿œ + å¸‚å ´åŒºåˆ†ãƒ»æ¥­ç¨®å–å¾—"""
        print(f"éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰æŠ½å‡ºé–‹å§‹: {df.shape}")
        print(f"åˆ—å: {list(df.columns)}")
        print("ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«:")
        print(df.head(3))

        symbols = {}  # ğŸ†• setã‹ã‚‰dictã«å¤‰æ›´
        import re

        # ğŸ†• åˆ—åã‚’æ¢ã™
        code_col = None
        name_col = None
        market_col = None
        sector_33_col = None
        sector_17_col = None
        size_col = None

        for col in df.columns:
            col_str = str(col)
            if 'ã‚³ãƒ¼ãƒ‰' in col_str and 'æ¥­ç¨®' not in col_str and 'è¦æ¨¡' not in col_str:
                if code_col is None:
                    code_col = col
            elif 'éŠ˜æŸ„å' in col_str:
                name_col = col
            elif 'å¸‚å ´' in col_str or 'Market' in col_str:
                market_col = col
            elif '33æ¥­ç¨®åŒºåˆ†' in col_str:
                sector_33_col = col
            elif '17æ¥­ç¨®åŒºåˆ†' in col_str:
                sector_17_col = col
            elif 'è¦æ¨¡åŒºåˆ†' in col_str:
                size_col = col

        print(f"\nğŸ” æ¤œå‡ºã•ã‚ŒãŸåˆ—:")
        print(f"  ã‚³ãƒ¼ãƒ‰åˆ—: {code_col}")
        print(f"  éŠ˜æŸ„ååˆ—: {name_col}")
        print(f"  å¸‚å ´åŒºåˆ†åˆ—: {market_col}")
        print(f"  33æ¥­ç¨®åŒºåˆ†åˆ—: {sector_33_col}")
        print(f"  17æ¥­ç¨®åŒºåˆ†åˆ—: {sector_17_col}")
        print(f"  è¦æ¨¡åŒºåˆ†åˆ—: {size_col}")

        # ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
        for idx, row in df.iterrows():
            try:
                # éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰å–å¾—
                if code_col is None:
                    continue
                    
                code_value = str(row[code_col]).strip()
                if not code_value or code_value == 'nan':
                    continue

                # 4æ¡æ•°å€¤ã®ã¿
                if code_value.isdigit() and len(code_value) == 4:
                    code = int(code_value)
                    if 1000 <= code <= 9999:
                        symbol = f"{code_value}.T"
                        
                        # ğŸ†• è¿½åŠ æƒ…å ±ã‚’å–å¾—
                        symbols[symbol] = {
                            'code': code_value,
                            'name': str(row[name_col]).strip() if name_col and pd.notna(row[name_col]) else 'N/A',
                            'market': str(row[market_col]).strip() if market_col and pd.notna(row[market_col]) else 'N/A',
                            'sector_33': str(row[sector_33_col]).strip() if sector_33_col and pd.notna(row[sector_33_col]) else 'N/A',
                            'sector_17': str(row[sector_17_col]).strip() if sector_17_col and pd.notna(row[sector_17_col]) else 'N/A',
                            'size': str(row[size_col]).strip() if size_col and pd.notna(row[size_col]) else 'N/A',
                        }
                
                # æ•°å­—+è‹±å­—ï¼ˆ130A, 1475BXãªã©ï¼‰
                elif re.match(r'^\d{3,4}[A-Z]{1,2}$', code_value):
                    symbol = f"{code_value}.T"
                    symbols[symbol] = {
                        'code': code_value,
                        'name': str(row[name_col]).strip() if name_col and pd.notna(row[name_col]) else 'N/A',
                        'market': str(row[market_col]).strip() if market_col and pd.notna(row[market_col]) else 'N/A',
                        'sector_33': str(row[sector_33_col]).strip() if sector_33_col and pd.notna(row[sector_33_col]) else 'N/A',
                        'sector_17': str(row[sector_17_col]).strip() if sector_17_col and pd.notna(row[sector_17_col]) else 'N/A',
                        'size': str(row[size_col]).strip() if size_col and pd.notna(row[size_col]) else 'N/A',
                    }

            except Exception as e:
                continue

        result = sorted(list(symbols.keys()))
        
        # çµ±è¨ˆè¡¨ç¤º
        alphabetic = [s for s in result if re.search(r'[A-Z]', s.replace('.T', ''))]
        numeric_only = [s for s in result if not re.search(r'[A-Z]', s.replace('.T', ''))]
        
        # ğŸ†• å¸‚å ´åŒºåˆ†åˆ¥ã®çµ±è¨ˆ
        markets = {}
        for symbol, info in symbols.items():
            market = info['market']
            if market not in markets:
                markets[market] = 0
            markets[market] += 1
        
        print(f"\næœ€çµ‚æŠ½å‡ºçµæœ: åˆè¨ˆ {len(result)} éŠ˜æŸ„")
        print(f"  æ•°å€¤ã®ã¿: {len(numeric_only)} éŠ˜æŸ„")
        if alphabetic:
            print(f"  è‹±å­—ä»˜ã: {len(alphabetic)} éŠ˜æŸ„")
        
        print(f"\nğŸ“Š å¸‚å ´åŒºåˆ†åˆ¥:")
        for market, count in sorted(markets.items()):
            print(f"  {market}: {count}éŠ˜æŸ„")

        if result:
            print(f"\næŠ½å‡ºä¾‹ï¼ˆè©³ç´°ï¼‰:")
            for symbol in result[:5]:
                info = symbols[symbol]
                print(f"  {symbol}: {info['name']} | {info['market']} | {info['sector_17']}")

        # ğŸ†• symbolsã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å¤‰æ•°ã«ä¿å­˜
        self.symbol_metadata = symbols

        return result

    def get_stock_data_safe(self, symbol):
        """å®‰å…¨ãªæ ªä¾¡ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆé…å½“æ—¥ãƒ»å‹•çš„æ™‚ä¾¡ç·é¡ãƒ»è¿½åŠ æŒ‡æ¨™å¯¾å¿œï¼‰"""
        try:
            time.sleep(self.config["request_delay"])

            ticker = yf.Ticker(symbol)
            data = ticker.history(period="5y")

            if data.empty or len(data) < 100:
                return None

            # VWAPè¨ˆç®—
            typical_price = (data['High'] + data['Low'] + data['Close']) / 3
            data['VWAP'] = (typical_price * data['Volume']).cumsum() / data['Volume'].cumsum()

            # ä¼æ¥­æƒ…å ±å–å¾—
            try:
                info = ticker.info
                shares_outstanding = info.get('sharesOutstanding', None)

                if shares_outstanding:
                    data['æ™‚ä¾¡ç·é¡'] = data['Close'] * shares_outstanding
                else:
                    current_market_cap = info.get('marketCap', None)
                    if current_market_cap:
                        current_price = data['Close'].iloc[-1]
                        estimated_shares = current_market_cap / current_price
                        data['æ™‚ä¾¡ç·é¡'] = data['Close'] * estimated_shares
                    else:
                        data['æ™‚ä¾¡ç·é¡'] = None

                # ğŸ†• 52é€±é«˜å€¤ãƒ»å®‰å€¤ã‚’è¨ˆç®—
                if len(data) >= 252:
                    week_52_high = data['High'].tail(252).max()
                    week_52_low = data['Low'].tail(252).min()
                else:
                    week_52_high = None
                    week_52_low = None

                company_info = {
                    'name': info.get('longName', info.get('shortName', 'N/A')),
                    'sector': info.get('sector', 'N/A'),
                    'industry': info.get('industry', 'N/A'),  # ğŸ†• æ¥­ç¨®è©³ç´°
                    'market_cap': info.get('marketCap', None),
                    'shares_outstanding': shares_outstanding,
                    
                    # ğŸ†• ãƒãƒªãƒ¥ã‚¨ãƒ¼ã‚·ãƒ§ãƒ³æŒ‡æ¨™
                    'trailing_pe': info.get('trailingPE', None),  # PER
                    'price_to_book': info.get('priceToBook', None),  # PBR
                    'beta': info.get('beta', None),  # ãƒ™ãƒ¼ã‚¿å€¤
                    'dividend_yield': info.get('dividendYield', None),  # é…å½“åˆ©å›ã‚Š
                    
                    # ğŸ†• 52é€±é«˜å€¤ãƒ»å®‰å€¤
                    'week_52_high': week_52_high,
                    'week_52_low': week_52_low,
                }
            except:
                data['æ™‚ä¾¡ç·é¡'] = None
                company_info = {
                    'name': 'N/A',
                    'sector': 'N/A',
                    'industry': 'N/A',
                    'market_cap': None,
                    'shares_outstanding': None,
                    'trailing_pe': None,
                    'price_to_book': None,
                    'beta': None,
                    'dividend_yield': None,
                    'week_52_high': None,
                    'week_52_low': None,
                }

            # é…å½“ãƒ‡ãƒ¼ã‚¿å–å¾—
            try:
                dividends = ticker.dividends
                five_years_ago = data.index[0]
                recent_dividends = dividends[dividends.index >= five_years_ago]

                data['é…å½“é‡‘é¡'] = 0.0
                data['é…å½“æ—¥'] = ''

                for div_date, div_amount in recent_dividends.items():
                    closest_date = data.index[data.index.get_indexer([div_date], method='nearest')[0]]
                    data.loc[closest_date, 'é…å½“é‡‘é¡'] = div_amount
                    data.loc[closest_date, 'é…å½“æ—¥'] = div_date.strftime('%Y-%m-%d')

            except Exception as div_error:
                data['é…å½“é‡‘é¡'] = 0.0
                data['é…å½“æ—¥'] = ''

            return {
                'price_data': data,
                'company_info': company_info
            }

        except Exception as e:
            if "Too Many Requests" not in str(e) and "Rate limited" not in str(e):
                print(f"{symbol} ã‚¨ãƒ©ãƒ¼: {e}")
            return None

    def upload_chunk_to_s3(self, chunk_symbols):
        """ãƒãƒ£ãƒ³ã‚¯ã‚’S3ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"""
        if not self.s3_client:
            return 0

        uploaded = 0

        for symbol in chunk_symbols:
            if symbol in self.stock_data:
                try:
                    clean_symbol = symbol.replace('.T', '')
                    price_data = self.stock_data[symbol]['price_data']

                    jp_data = price_data.rename(columns={
                        'Open': 'å§‹å€¤',
                        'High': 'é«˜å€¤',
                        'Low': 'å®‰å€¤',
                        'Close': 'çµ‚å€¤',
                        'Volume': 'å‡ºæ¥é«˜',
                        'VWAP': 'VWAP'
                    })

                    csv_buffer = io.StringIO()
                    jp_data.to_csv(csv_buffer, encoding='utf-8-sig')

                    s3_key = f"japan-stocks-5years-chart/stocks/{clean_symbol}.csv"

                    self.s3_client.put_object(
                        Bucket="m-s3storage",
                        Key=s3_key,
                        Body=csv_buffer.getvalue().encode('utf-8'),
                        ContentType='text/csv'
                    )

                    uploaded += 1

                except Exception as e:
                    pass

        return uploaded

    def collect_all_stocks(self):
        """å…¨éŠ˜æŸ„åé›†"""
        if not self.jpx_symbols:
            print("éŠ˜æŸ„ãƒªã‚¹ãƒˆãªã—")
            return False

        total = len(self.jpx_symbols)
        print(f"ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹: {total} éŠ˜æŸ„")
        print(f"ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–: {self.config['request_delay']}ç§’é–“éš”, {self.config['max_workers']}ä¸¦åˆ—")

        chunk_size = self.config["chunk_size"]
        chunks = [self.jpx_symbols[i:i+chunk_size] for i in range(0, len(self.jpx_symbols), chunk_size)]

        success_count = 0
        total_uploaded = 0

        for chunk_idx, chunk in enumerate(chunks):
            print(f"\nãƒãƒ£ãƒ³ã‚¯ {chunk_idx + 1}/{len(chunks)} å‡¦ç†ä¸­ ({len(chunk)} éŠ˜æŸ„)")

            chunk_success = 0
            chunk_start_time = time.time()

            with tqdm(total=len(chunk), desc=f"ãƒãƒ£ãƒ³ã‚¯{chunk_idx + 1}") as pbar:
                with ThreadPoolExecutor(max_workers=self.config["max_workers"]) as executor:
                    futures = {executor.submit(self.get_stock_data_safe, symbol): symbol for symbol in chunk}

                    for future in as_completed(futures):
                        symbol = futures[future]
                        result = future.result()

                        if result:
                            with self.lock:
                                self.stock_data[symbol] = result
                                success_count += 1
                                chunk_success += 1
                        else:
                            with self.lock:
                                self.failed_symbols.append(symbol)

                        pbar.update(1)
                        pbar.set_postfix({
                            'ãƒãƒ£ãƒ³ã‚¯æˆåŠŸ': chunk_success,
                            'ç·æˆåŠŸ': success_count,
                            'æˆåŠŸç‡': f"{success_count/(success_count + len(self.failed_symbols))*100:.1f}%"
                        })

            chunk_time = time.time() - chunk_start_time

            if self.s3_client and chunk_success > 0:
                uploaded = self.upload_chunk_to_s3(chunk)
                total_uploaded += uploaded
                print(f"S3ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†: {uploaded} ãƒ•ã‚¡ã‚¤ãƒ« (ç´¯è¨ˆ: {total_uploaded})")

            remaining_chunks = len(chunks) - chunk_idx - 1
            estimated_remaining = remaining_chunks * (chunk_time + self.config["chunk_delay"]) / 60
            print(f"ãƒãƒ£ãƒ³ã‚¯{chunk_idx + 1}å®Œäº†: {chunk_success}/{len(chunk)} æˆåŠŸ")
            print(f"æ®‹ã‚Šæ¨å®šæ™‚é–“: {estimated_remaining:.1f} åˆ†")

            if chunk_idx < len(chunks) - 1:
                print(f"ä¼‘æ†©ä¸­... {self.config['chunk_delay']} ç§’")
                time.sleep(self.config["chunk_delay"])

        print(f"\nåé›†å®Œäº†:")
        print(f"  æˆåŠŸ: {success_count} éŠ˜æŸ„")
        print(f"  å¤±æ•—: {len(self.failed_symbols)} éŠ˜æŸ„")
        print(f"  æˆåŠŸç‡: {success_count/(success_count + len(self.failed_symbols))*100:.1f}%")
        print(f"  S3ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰: {total_uploaded} ãƒ•ã‚¡ã‚¤ãƒ«")

        if self.s3_client and self.stock_data:
            self._save_summary_to_s3()

        return True

    def _save_summary_to_s3(self):
        """ã‚µãƒãƒªãƒ¼ã‚’S3ä¿å­˜ - çµ±è¨ˆæŒ‡æ¨™ãƒ»å¸‚å ´åŒºåˆ†è¿½åŠ ç‰ˆ"""
        try:
            summary_data = []

            for symbol, data_info in self.stock_data.items():
                price_data = data_info['price_data']
                company_info = data_info['company_info']
                
                # ğŸ†• JPXãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—
                jpx_meta = self.symbol_metadata.get(symbol, {})

                # åŸºæœ¬æƒ…å ±
                first_price = price_data['Close'].iloc[0]
                latest_price = price_data['Close'].iloc[-1]
                total_return = (latest_price / first_price - 1) * 100

                # é…å½“æƒ…å ±
                dividend_count = len(price_data[price_data['é…å½“é‡‘é¡'] > 0])
                total_dividends = price_data['é…å½“é‡‘é¡'].sum()

                # ğŸ†• çµ±è¨ˆæŒ‡æ¨™è¨ˆç®—
                try:
                    returns = price_data['Close'].pct_change().dropna()
                    returns = returns.replace([np.inf, -np.inf], np.nan).dropna()
                    
                    if len(returns) > 50:
                        # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆå¹´ç‡ï¼‰
                        volatility = returns.std() * np.sqrt(252)
                        
                        # å¹´ç‡ãƒªã‚¿ãƒ¼ãƒ³
                        annual_return = returns.mean() * 252
                        
                        # ã‚·ãƒ£ãƒ¼ãƒ—ãƒ¬ã‚·ã‚ª
                        sharpe_ratio = annual_return / volatility if volatility > 0 else 0
                        
                        # æœ€å¤§ãƒ‰ãƒ­ãƒ¼ãƒ€ã‚¦ãƒ³
                        cumulative = (1 + returns).cumprod()
                        running_max = cumulative.expanding().max()
                        drawdown = (cumulative - running_max) / running_max
                        max_drawdown = drawdown.min()
                        
                        # å¹³å‡å£²è²·ä»£é‡‘
                        trading_values = price_data['Close'] * price_data['Volume']
                        avg_trading_value = trading_values.mean()
                    else:
                        volatility = None
                        annual_return = None
                        sharpe_ratio = None
                        max_drawdown = None
                        avg_trading_value = None
                        
                except Exception as e:
                    volatility = None
                    annual_return = None
                    sharpe_ratio = None
                    max_drawdown = None
                    avg_trading_value = None

                # ğŸ†• é…å½“åˆ©å›ã‚Šè¨ˆç®—
                if company_info.get('dividend_yield') is None and total_dividends > 0:
                    annual_dividend = total_dividends / 5
                    calculated_yield = (annual_dividend / latest_price) if latest_price > 0 else 0
                else:
                    calculated_yield = company_info.get('dividend_yield')

                summary_data.append({
                    'éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰': symbol.replace('.T', ''),
                    'ä¼šç¤¾å': jpx_meta.get('name', company_info.get('name', 'N/A')),
                    
                    # ğŸ†• JPXæƒ…å ±
                    'å¸‚å ´åŒºåˆ†': jpx_meta.get('market', 'N/A'),
                    '33æ¥­ç¨®åŒºåˆ†': jpx_meta.get('sector_33', 'N/A'),
                    '17æ¥­ç¨®åŒºåˆ†': jpx_meta.get('sector_17', 'N/A'),
                    'è¦æ¨¡åŒºåˆ†': jpx_meta.get('size', 'N/A'),
                    
                    # yfinanceæƒ…å ±
                    'ã‚»ã‚¯ã‚¿ãƒ¼ï¼ˆYFï¼‰': company_info.get('sector', 'N/A'),
                    'æ¥­ç¨®ï¼ˆYFï¼‰': company_info.get('industry', 'N/A'),
                    
                    # æœŸé–“æƒ…å ±
                    'æœŸé–“é–‹å§‹': str(price_data.index[0].date()),
                    'æœŸé–“çµ‚äº†': str(price_data.index[-1].date()),
                    'ãƒ‡ãƒ¼ã‚¿æ—¥æ•°': len(price_data),
                    
                    # ä¾¡æ ¼æƒ…å ±
                    'é–‹å§‹ä¾¡æ ¼': round(first_price, 2) if first_price else None,
                    'æœ€æ–°ä¾¡æ ¼': round(latest_price, 2) if latest_price else None,
                    '52é€±é«˜å€¤': round(company_info.get('week_52_high'), 2) if company_info.get('week_52_high') else None,
                    '52é€±å®‰å€¤': round(company_info.get('week_52_low'), 2) if company_info.get('week_52_low') else None,
                    
                    # ãƒªã‚¿ãƒ¼ãƒ³æŒ‡æ¨™
                    '5å¹´å¤‰åŒ–ç‡(%)': round(total_return, 2) if total_return else None,
                    'å¹´ç‡ãƒªã‚¿ãƒ¼ãƒ³(%)': round(annual_return * 100, 2) if annual_return is not None and not np.isnan(annual_return) else None,
                    
                    # ãƒªã‚¹ã‚¯æŒ‡æ¨™
                    'ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£(%)': round(volatility * 100, 2) if volatility is not None and not np.isnan(volatility) else None,
                    'ã‚·ãƒ£ãƒ¼ãƒ—ãƒ¬ã‚·ã‚ª': round(sharpe_ratio, 2) if sharpe_ratio is not None and not np.isnan(sharpe_ratio) else None,
                    'æœ€å¤§DD(%)': round(max_drawdown * 100, 2) if max_drawdown is not None and not np.isnan(max_drawdown) else None,
                    
                    # ãƒãƒªãƒ¥ã‚¨ãƒ¼ã‚·ãƒ§ãƒ³
                    'PER': round(company_info.get('trailing_pe'), 2) if company_info.get('trailing_pe') else None,
                    'PBR': round(company_info.get('price_to_book'), 2) if company_info.get('price_to_book') else None,
                    'ãƒ™ãƒ¼ã‚¿': round(company_info.get('beta'), 2) if company_info.get('beta') else None,
                    
                    # æµå‹•æ€§æŒ‡æ¨™
                    'å¹³å‡å‡ºæ¥é«˜': int(price_data['Volume'].mean()) if not price_data['Volume'].isna().all() else None,
                    'å¹³å‡å£²è²·ä»£é‡‘': int(avg_trading_value) if avg_trading_value is not None and not np.isnan(avg_trading_value) else None,
                    
                    # é…å½“æƒ…å ±
                    'é…å½“å›æ•°': dividend_count,
                    'ç·é…å½“é¡': round(total_dividends, 2) if total_dividends > 0 else 0,
                    'é…å½“åˆ©å›ã‚Š(%)': round(calculated_yield * 100, 2) if calculated_yield and not np.isnan(calculated_yield) else None,
                    
                    # ä¼æ¥­æƒ…å ±
                    'æœ€æ–°æ™‚ä¾¡ç·é¡': price_data['æ™‚ä¾¡ç·é¡'].iloc[-1] if price_data['æ™‚ä¾¡ç·é¡'].iloc[-1] else None
                })

            summary_df = pd.DataFrame(summary_data)
            summary_csv = summary_df.to_csv(index=False, encoding='utf-8-sig')

            self.s3_client.put_object(
                Bucket="m-s3storage",
                Key="japan-stocks-5years-chart/summary.csv",
                Body=summary_csv.encode('utf-8'),
                ContentType='text/csv'
            )

            print("ã‚µãƒãƒªãƒ¼ä¿å­˜å®Œäº†: summary.csv")

            # çµ±è¨ˆè¡¨ç¤º
            total_stocks = len(summary_df)
            positive_returns = len(summary_df[summary_df['5å¹´å¤‰åŒ–ç‡(%)'] > 0])
            dividend_stocks = len(summary_df[summary_df['é…å½“å›æ•°'] > 0])

            print(f"\næœ€çµ‚çµ±è¨ˆ:")
            print(f"  ç·éŠ˜æŸ„æ•°: {total_stocks}")
            print(f"  ãƒ—ãƒ©ã‚¹ãƒªã‚¿ãƒ¼ãƒ³: {positive_returns}/{total_stocks} ({positive_returns/total_stocks*100:.1f}%)")
            print(f"  é…å½“æ”¯æ‰•ã„éŠ˜æŸ„: {dividend_stocks}/{total_stocks} ({dividend_stocks/total_stocks*100:.1f}%)")
            
            if len(summary_df) > 0:
                print(f"  å¹³å‡5å¹´ãƒªã‚¿ãƒ¼ãƒ³: {summary_df['5å¹´å¤‰åŒ–ç‡(%)'].mean():.2f}%")
                print(f"  å¹³å‡å¹´ç‡ãƒªã‚¿ãƒ¼ãƒ³: {summary_df['å¹´ç‡ãƒªã‚¿ãƒ¼ãƒ³(%)'].mean():.2f}%")
                print(f"  å¹³å‡ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£: {summary_df['ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£(%)'].mean():.2f}%")
                print(f"  å¹³å‡ã‚·ãƒ£ãƒ¼ãƒ—ãƒ¬ã‚·ã‚ª: {summary_df['ã‚·ãƒ£ãƒ¼ãƒ—ãƒ¬ã‚·ã‚ª'].mean():.2f}")
                print(f"  å¹³å‡é…å½“å›æ•°: {summary_df['é…å½“å›æ•°'].mean():.1f}å›")

        except Exception as e:
            print(f"ã‚µãƒãƒªãƒ¼ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()


def run_safe_collection():
    """ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–ç‰ˆå®Ÿè¡Œ"""
    print("JPXå…¨éŠ˜æŸ„5å¹´åˆ†æ ªä¾¡ãƒ‡ãƒ¼ã‚¿åé›† - é…å½“æ—¥ãƒ»å‹•çš„æ™‚ä¾¡ç·é¡å¯¾å¿œç‰ˆ")
    print("=" * 70)

    collector = JPXStockCollector()

    symbols = collector.get_jpx_symbols()
    if not symbols:
        print("éŠ˜æŸ„å–å¾—å¤±æ•—")
        return None

    collector.jpx_symbols = symbols
    print(f"å¯¾è±¡éŠ˜æŸ„: {len(symbols)} ä»¶")

    s3_ok = collector.setup_s3()
    if not s3_ok:
        print("S3æ¥ç¶šå¤±æ•—")
        return None

    estimated_time = (len(symbols) / collector.config["chunk_size"]) * (
        collector.config["chunk_size"] * collector.config["request_delay"] / collector.config["max_workers"] +
        collector.config["chunk_delay"]
    ) / 60

    print(f"\nå®Ÿè¡Œç¢ºèª:")
    print(f"  å¯¾è±¡éŠ˜æŸ„æ•°: {len(symbols):,}")
    print(f"  æ¨å®šå‡¦ç†æ™‚é–“: {estimated_time:.1f} åˆ†")

    start_time = time.time()
    success = collector.collect_all_stocks()
    elapsed = time.time() - start_time

    print(f"\nå‡¦ç†å®Œäº†: {elapsed/60:.1f} åˆ†")
    print(f"S3ä¿å­˜å…ˆ: s3://m-s3storage/japan-stocks-5years-chart/stocks/")

    return collector


if __name__ == "__main__":
    try:
        run_safe_collection()
        status = "âœ… æˆåŠŸ"
        color = "good"
    except Exception as e:
        status = f"âŒ å¤±æ•—: {str(e)}"
        color = "danger"
    
    slack_webhook_url = os.getenv("SLACK_WEBHOOK_URL")
    
    if slack_webhook_url:
        message = {
            "attachments": [{
                "color": color,
                "title": "ãƒ‡ãƒ¼ã‚¿åé›†å®Œäº†",
                "text": status,
                "footer": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }]
        }
        requests.post(slack_webhook_url, json=message)
    else:
        print("è­¦å‘Š: SLACK_WEBHOOK_URLãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
