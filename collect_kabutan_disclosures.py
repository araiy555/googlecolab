#!/usr/bin/env python3
"""
æ ªæ¢é–‹ç¤ºæƒ…å ±æ—¥æ¬¡åé›†ã‚·ã‚¹ãƒ†ãƒ ï¼ˆä¿®æ­£ç‰ˆï¼‰
ä¿®æ­£ç‚¹:
  1. ãƒ†ãƒ¼ãƒ–ãƒ«ã®ã‚«ãƒ©ãƒ ä½ç½®ãƒ™ãƒ¼ã‚¹ã§ãƒ‘ãƒ¼ã‚¹ï¼ˆä¼šç¤¾åã‚¹ã‚­ãƒƒãƒ—å•é¡Œã‚’ä¿®æ­£ï¼‰
  2. æ—¥ä»˜ãƒ‘ãƒ¼ã‚¹ã‚’ YY/MM/DD HH:MM å½¢å¼ã«å¯¾å¿œï¼ˆæ—¥ã®ç²¾åº¦å–ªå¤±ã‚’ä¿®æ­£ï¼‰
  3. parse_disclosure_item ã®ã‚´ãƒŸHTMLæ··å…¥ã‚’é˜²æ­¢
"""

import requests
import boto3
import json
import os
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import time
import re
import random
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry


class KabutanDailyCollector:
    def __init__(self):
        """æ—¥æ¬¡åé›†ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–"""
        print("æ—¥æ¬¡åé›†ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ä¸­...")

        self.base_url = "https://kabutan.jp"
        self.disclosure_url = "https://kabutan.jp/disclosures/"
        self.bucket_name = "m-s3storage"
        self.s3_prefix = "japan-stocks-5years-chart/monthly-disclosures/"

        self.delay_base = 1.0
        self.delay_variance = 0.3
        self.retry_delay = 5.0
        self.max_retries = 3
        self.max_consecutive_empty = 20
        self.session_reset_interval = 200

        self.s3 = self._init_s3_client()
        self.session = None
        self.request_count = 0
        self._init_session()

        self.stats = {
            'requests': 0,
            'success': 0,
            'total_disclosures': 0,
            'start_time': None
        }

        print("åˆæœŸåŒ–å®Œäº†")

    def _init_s3_client(self):
        try:
            aws_access_key = os.getenv('AWS_ACCESS_KEY_ID')
            aws_secret_key = os.getenv('AWS_SECRET_ACCESS_KEY')
            if aws_access_key and aws_secret_key:
                return boto3.client('s3',
                    aws_access_key_id=aws_access_key,
                    aws_secret_access_key=aws_secret_key,
                    region_name="ap-northeast-1")
            else:
                return boto3.client('s3', region_name="ap-northeast-1")
        except Exception as e:
            print(f"S3ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–å¤±æ•—: {e}")
            return None

    def _init_session(self):
        self.session = requests.Session()
        retry_strategy = Retry(
            total=2, backoff_factor=0.5,
            status_forcelist=[429, 500, 502, 503, 504])
        adapter = HTTPAdapter(max_retries=retry_strategy)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)

        user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        ]
        self.session.headers.update({
            'User-Agent': random.choice(user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9',
            'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
            'Connection': 'keep-alive',
        })

    # =========================================================================
    #  ğŸ”§ ä¿®æ­£1: æ—¥ä»˜ãƒ‘ãƒ¼ã‚¹
    # =========================================================================
    def parse_datetime_str(self, text, year):
        """
        æ ªæ¢ã®æ—¥æ™‚æ–‡å­—åˆ—ã‚’ãƒ‘ãƒ¼ã‚¹
        å¯¾å¿œãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: "YY/MM/DD HH:MM" (ä¾‹: "20/04/13 15:30")
        
        Returns:
            tuple: (date_str "YYYY-MM-DD", time_str "HH:MM") or (None, None)
        """
        # YY/MM/DD HH:MM ãƒ‘ã‚¿ãƒ¼ãƒ³
        match = re.search(r'(\d{2})/(\d{2})/(\d{2})\s+(\d{1,2}:\d{2})', text)
        if match:
            yy = int(match.group(1))
            mm = int(match.group(2))
            dd = int(match.group(3))
            time_str = match.group(4)
            
            # 2æ¡å¹´ â†’ 4æ¡å¹´ (00-99 â†’ 2000-2099)
            full_year = 2000 + yy
            
            # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
            if 1 <= mm <= 12 and 1 <= dd <= 31:
                return f"{full_year}-{mm:02d}-{dd:02d}", time_str
        
        # YY/MM/DD ã®ã¿ï¼ˆæ™‚åˆ»ãªã—ï¼‰
        match = re.search(r'(\d{2})/(\d{2})/(\d{2})', text)
        if match:
            yy = int(match.group(1))
            mm = int(match.group(2))
            dd = int(match.group(3))
            full_year = 2000 + yy
            if 1 <= mm <= 12 and 1 <= dd <= 31:
                return f"{full_year}-{mm:02d}-{dd:02d}", None
        
        return None, None

    # =========================================================================
    #  ğŸ”§ ä¿®æ­£2: ãƒ†ãƒ¼ãƒ–ãƒ«è¡Œãƒ‘ãƒ¼ã‚¹ï¼ˆã‚«ãƒ©ãƒ ä½ç½®ãƒ™ãƒ¼ã‚¹ï¼‰
    # =========================================================================
    def parse_disclosure_row(self, cells, year, month):
        """
        ãƒ†ãƒ¼ãƒ–ãƒ«è¡Œã‹ã‚‰é–‹ç¤ºæƒ…å ±ã‚’ãƒ‘ãƒ¼ã‚¹ï¼ˆã‚«ãƒ©ãƒ ä½ç½®ãƒ™ãƒ¼ã‚¹ï¼‰
        
        æ ªæ¢ã®ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹é€ :
          [0] ã‚³ãƒ¼ãƒ‰  [1] ä¼šç¤¾å  [2] å¸‚å ´  [3] æƒ…å ±ç¨®åˆ¥  [4] ã‚¿ã‚¤ãƒˆãƒ«  [5] é–‹ç¤ºæ—¥æ™‚
        
        â€» ã‚»ãƒ«æ•°ãŒ6æœªæº€ã®å ´åˆã¯ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œãªã©ãªã®ã§ã‚¹ã‚­ãƒƒãƒ—
        """
        try:
            texts = [cell.get_text().strip() for cell in cells]

            # 6åˆ—ï¼ˆã‚³ãƒ¼ãƒ‰/ä¼šç¤¾å/å¸‚å ´/æƒ…å ±ç¨®åˆ¥/ã‚¿ã‚¤ãƒˆãƒ«/é–‹ç¤ºæ—¥æ™‚ï¼‰ã‚’æœŸå¾…
            if len(texts) < 6:
                return None

            # --- ã‚«ãƒ©ãƒ 0: éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰ ---
            stock_code_text = texts[0]
            code_match = re.match(r'^(\d{4})$', stock_code_text)
            if not code_match:
                return None
            stock_code = code_match.group(1)
            if not (1000 <= int(stock_code) <= 9999):
                return None

            # --- ã‚«ãƒ©ãƒ 1: ä¼šç¤¾å ---
            company_name = texts[1].strip() if texts[1].strip() else "ä¸æ˜"

            # --- ã‚«ãƒ©ãƒ 2: å¸‚å ´ï¼ˆä½¿ã‚ãªã„ãŒè¨˜éŒ²å¯ï¼‰ ---
            # market = texts[2].strip()

            # --- ã‚«ãƒ©ãƒ 3: æƒ…å ±ç¨®åˆ¥ ---
            info_type = texts[3].strip()

            # --- ã‚«ãƒ©ãƒ 4: ã‚¿ã‚¤ãƒˆãƒ« ---
            title = texts[4].strip() if texts[4].strip() else "ä¸æ˜"

            # --- ã‚«ãƒ©ãƒ 5: é–‹ç¤ºæ—¥æ™‚ ---
            datetime_text = texts[5].strip()
            date_str, time_str = self.parse_datetime_str(datetime_text, year)

            if not date_str:
                # æ—¥æ™‚ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ãŸå ´åˆã€æœˆåˆã‚’ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                date_str = f"{year}-{month:02d}-01"

            # ã‚«ãƒ†ã‚´ãƒªåˆ†é¡
            category = self.categorize_disclosure(title)

            return {
                'stock_code': stock_code,
                'company_name': company_name,
                'title': title,
                'date': date_str,
                'datetime': f"{date_str} {time_str}" if time_str else date_str,
                'category': category,
                'info_type': info_type,  # æƒ…å ±ç¨®åˆ¥ã‚‚ä¿æŒ
                'source': 'æ ªæ¢',
                'year': int(date_str[:4]),
                'month': int(date_str[5:7])
            }

        except Exception as e:
            # ãƒ‡ãƒãƒƒã‚°ç”¨: ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼ã‚’ãƒ­ã‚°
            # print(f"  parse_disclosure_row error: {e}")
            pass

        return None

    # =========================================================================
    #  ğŸ”§ ä¿®æ­£3: parse_disclosure_item ã®ã‚´ãƒŸé™¤å¤–
    # =========================================================================
    def parse_disclosure_item(self, item, year, month):
        """
        ã‚¢ã‚¤ãƒ†ãƒ ã‹ã‚‰é–‹ç¤ºæƒ…å ±ã‚’ãƒ‘ãƒ¼ã‚¹
        
        ä¿®æ­£: ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³è¦ç´ ãƒ»ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’é™¤å¤–
        """
        try:
            text = item.get_text().strip()

            # --- ã‚´ãƒŸé™¤å¤–ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ ---
            # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³æ–‡å­—åˆ—
            if any(keyword in text for keyword in ['æ¬¡ã¸', 'å‰ã¸', 'ï¼Â»', 'Â«ï¼œ']):
                return None
            
            # ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ˜ãƒƒãƒ€ãƒ¼
            if any(keyword in text for keyword in ['ã‚³ãƒ¼ãƒ‰\n', 'ä¼šç¤¾å\n', 'é–‹ç¤ºæ—¥æ™‚\n']):
                return None
            
            # çŸ­ã™ãã‚‹ãƒ†ã‚­ã‚¹ãƒˆ
            if len(text) < 15:
                return None

            # éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰æŠ½å‡º
            codes = re.findall(r'\b(\d{4})\b', text)
            stock_code = None
            for code in codes:
                if 1000 <= int(code) <= 9999:
                    stock_code = code
                    break

            if not stock_code:
                return None

            # ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ—¥æ™‚ã‚’æŠ½å‡ºè©¦è¡Œ
            date_str, time_str = self.parse_datetime_str(text, year)
            if not date_str:
                date_str = f"{year}-{month:02d}-01"

            return {
                'stock_code': stock_code,
                'company_name': "æŠ½å‡ºä¸­",
                'title': text[:200],
                'date': date_str,
                'category': self.categorize_disclosure(text),
                'source': 'æ ªæ¢',
                'year': year,
                'month': month
            }

        except Exception:
            pass

        return None

    # =========================================================================
    #  ãƒšãƒ¼ã‚¸è§£æï¼ˆä¿®æ­£: ãƒ†ãƒ¼ãƒ–ãƒ«ã®ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’æ¤œå‡ºã—ã¦ã‚«ãƒ©ãƒ é †ã‚’ç¢ºèªï¼‰
    # =========================================================================
    def extract_disclosures_from_page(self, soup, year, month):
        """ãƒšãƒ¼ã‚¸ã‹ã‚‰é–‹ç¤ºæƒ…å ±ã‚’æŠ½å‡º"""
        disclosures = []

        # ãƒ¡ã‚¤ãƒ³ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰ã®æŠ½å‡º
        tables = soup.find_all('table')
        for table in tables:
            rows = table.find_all('tr')
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’æ¤œå‡ºã—ã¦æ­£ã—ã„ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ç¢ºèª
            is_disclosure_table = False
            for row in rows:
                header_cells = row.find_all('th')
                if header_cells:
                    header_text = ' '.join(cell.get_text().strip() for cell in header_cells)
                    if 'ã‚³ãƒ¼ãƒ‰' in header_text and 'ã‚¿ã‚¤ãƒˆãƒ«' in header_text:
                        is_disclosure_table = True
                        break
            
            if not is_disclosure_table:
                continue
            
            for row in rows:
                cells = row.find_all(['td'])  # th ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆãƒ‡ãƒ¼ã‚¿è¡Œã®ã¿ï¼‰
                if len(cells) >= 6:
                    row_data = self.parse_disclosure_row(cells, year, month)
                    if row_data:
                        disclosures.append(row_data)

        # parse_disclosure_item ã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆãƒ†ãƒ¼ãƒ–ãƒ«ä»¥å¤–ã®æ§‹é€ ç”¨ï¼‰
        # ãŸã ã—ã€ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰å–å¾—ã§ããŸå ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
        if not disclosures:
            disclosure_items = soup.find_all(['div', 'li'], 
                                            class_=re.compile(r'disclosure|item|news'))
            for item in disclosure_items:
                item_data = self.parse_disclosure_item(item, year, month)
                if item_data:
                    disclosures.append(item_data)

        return disclosures

    # =========================================================================
    #  ä»¥ä¸‹ã¯å¤‰æ›´ãªã—ï¼ˆã‚«ãƒ†ã‚´ãƒªåˆ†é¡ã€S3ä¿å­˜ã€åé›†ãƒ­ã‚¸ãƒƒã‚¯ç­‰ï¼‰
    # =========================================================================
    def categorize_disclosure(self, title):
        if any(w in title for w in ['æ±ºç®—', 'æ¥­ç¸¾', 'å››åŠæœŸ', 'å£²ä¸Š', 'åˆ©ç›Š']):
            return 'æ±ºç®—ãƒ»æ¥­ç¸¾'
        elif any(w in title for w in ['é…å½“', 'æ ªä¸»å„ªå¾…', 'è‡ªå·±æ ªå¼']):
            return 'é…å½“ãƒ»æ ªä¸»é‚„å…ƒ'
        elif any(w in title for w in ['äººäº‹', 'å½¹å“¡', 'ä»£è¡¨å–ç· å½¹']):
            return 'äººäº‹ãƒ»çµ„ç¹”'
        elif any(w in title for w in ['è²·å', 'M&A', 'è³‡æœ¬ææº', 'æ¥­å‹™ææº']):
            return 'M&Aãƒ»ææº'
        elif any(w in title for w in ['æ–°è£½å“', 'æ–°ã‚µãƒ¼ãƒ“ã‚¹', 'é–‹ç™º', 'ç‰¹è¨±']):
            return 'äº‹æ¥­ãƒ»è£½å“'
        else:
            return 'ãã®ä»–'

    def load_existing_month_data(self, year, month):
        if not self.s3:
            return None
        try:
            key = f"{self.s3_prefix}{year}-{month:02d}.json"
            response = self.s3.get_object(Bucket=self.bucket_name, Key=key)
            return json.loads(response['Body'].read().decode('utf-8'))
        except:
            return None

    def fetch_month_disclosures(self, year, month):
        all_disclosures = []
        date_param = f"{year}{month:02d}00"
        print(f"å–å¾—é–‹å§‹: {year}å¹´{month}æœˆ (date={date_param})")

        try:
            page = 1
            consecutive_empty = 0

            while consecutive_empty < self.max_consecutive_empty:
                if self.request_count >= self.session_reset_interval:
                    self._init_session()
                    self.request_count = 0

                url = f"{self.disclosure_url}?kubun=&date={date_param}&page={page}"

                success = False
                for retry in range(self.max_retries):
                    try:
                        print(f"  ãƒšãƒ¼ã‚¸{page}", end='', flush=True)
                        delay = self.delay_base + random.uniform(-self.delay_variance, self.delay_variance)
                        if retry > 0:
                            delay += self.retry_delay
                        time.sleep(delay)

                        response = self.session.get(url, timeout=30)
                        self.stats['requests'] += 1
                        self.request_count += 1

                        if response.status_code == 200:
                            self.stats['success'] += 1
                            success = True
                            break
                        elif response.status_code == 429:
                            print(" - ãƒ¬ãƒ¼ãƒˆåˆ¶é™", flush=True)
                            time.sleep(10)
                        else:
                            print(f" - HTTPã‚¨ãƒ©ãƒ¼: {response.status_code}", flush=True)
                            time.sleep(self.retry_delay)
                    except Exception as e:
                        print(f" - ã‚¨ãƒ©ãƒ¼: {e}", flush=True)
                        time.sleep(self.retry_delay)

                if not success:
                    consecutive_empty += 1
                    page += 1
                    print(" - å¤±æ•—", flush=True)
                    continue

                soup = BeautifulSoup(response.content, 'html.parser')
                page_disclosures = self.extract_disclosures_from_page(soup, year, month)

                if page_disclosures:
                    consecutive_empty = 0
                    all_disclosures.extend(page_disclosures)
                    print(f" - {len(page_disclosures)}ä»¶", flush=True)
                else:
                    consecutive_empty += 1
                    print(" - 0ä»¶", flush=True)

                page += 1
                if page > 1000:
                    print(f"  æœ€å¤§ãƒšãƒ¼ã‚¸æ•°åˆ°é”: {page-1}")
                    break

            print(f"å®Œäº†: {year}å¹´{month}æœˆ - {len(all_disclosures)}ä»¶")
            return all_disclosures

        except Exception as e:
            print(f"ã‚¨ãƒ©ãƒ¼: {year}å¹´{month}æœˆ - {e}")
            return []

    def save_month_to_s3(self, year, month, disclosures):
        if not self.s3:
            print("S3ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
            return False
        try:
            month_data = {
                'year': year,
                'month': month,
                'total_disclosures': len(disclosures),
                'disclosures': disclosures,
                'categories': {},
                'companies': {},
                'updated_at': datetime.now().isoformat()
            }
            for d in disclosures:
                cat = d.get('category', 'ãã®ä»–')
                month_data['categories'][cat] = month_data['categories'].get(cat, 0) + 1
                comp = d.get('company_name', 'ä¸æ˜')
                month_data['companies'][comp] = month_data['companies'].get(comp, 0) + 1

            key = f"{self.s3_prefix}{year}-{month:02d}.json"
            self.s3.put_object(
                Bucket=self.bucket_name, Key=key,
                Body=json.dumps(month_data, ensure_ascii=False, indent=2).encode('utf-8'),
                ContentType='application/json')
            print(f"S3ä¿å­˜å®Œäº†: {key}")
            return True
        except Exception as e:
            print(f"S3ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def get_target_months(self):
        today = datetime.now()
        current_year = today.year
        current_month = today.month
        months = []
        if current_month == 1:
            months.append((current_year - 1, 12))
        else:
            months.append((current_year, current_month - 1))
        months.append((current_year, current_month))
        return months

    def run_daily_collection(self):
        print("=" * 60)
        print("æ ªæ¢æ—¥æ¬¡é–‹ç¤ºæƒ…å ±åé›†é–‹å§‹")
        print("=" * 60)
        self.stats['start_time'] = time.time()

        target_months = self.get_target_months()
        print(f"å¯¾è±¡æœˆ: {target_months[0][0]}å¹´{target_months[0][1]}æœˆ ã¨ "
              f"{target_months[1][0]}å¹´{target_months[1][1]}æœˆ")
        print("=" * 60)

        total_new = 0
        try:
            for year, month in target_months:
                print(f"\nå‡¦ç†: {year}å¹´{month}æœˆ")
                existing_data = self.load_existing_month_data(year, month)
                existing_count = len(existing_data.get('disclosures', [])) if existing_data else 0

                new_disclosures = self.fetch_month_disclosures(year, month)
                if new_disclosures:
                    if self.save_month_to_s3(year, month, new_disclosures):
                        new_count = len(new_disclosures)
                        diff = new_count - existing_count
                        total_new += new_count
                        print(f"  çµæœ: {new_count}ä»¶ (å‰å›æ¯”: {diff:+d}ä»¶)")
                else:
                    print("  çµæœ: 0ä»¶")

            elapsed = time.time() - self.stats['start_time']
            print("\n" + "=" * 60)
            print("åé›†å®Œäº†")
            print(f"ç·é–‹ç¤ºä»¶æ•°: {total_new}ä»¶")
            print(f"å‡¦ç†æ™‚é–“: {elapsed:.1f}ç§’")
            print("=" * 60)
            return True
        except Exception as e:
            print(f"ã‚¨ãƒ©ãƒ¼: {e}")
            return False


def notify_slack(status, message):
    slack_webhook_url = os.getenv("SLACK_WEBHOOK_URL")
    if not slack_webhook_url:
        print("è­¦å‘Š: SLACK_WEBHOOK_URLãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return
    color = "good" if status == "success" else "danger"
    emoji = "âœ…" if status == "success" else "âŒ"
    payload = {
        "attachments": [{
            "color": color,
            "title": f"{emoji} æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿åé›†",
            "text": message,
            "footer": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }]
    }
    try:
        requests.post(slack_webhook_url, json=payload)
    except Exception as e:
        print(f"Slacké€šçŸ¥ã‚¨ãƒ©ãƒ¼: {e}")


def main():
    try:
        collector = KabutanDailyCollector()
        collector.run_daily_collection()
        print("æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿åé›†ãŒå®Œäº†ã—ã¾ã—ãŸ")
        notify_slack("success", "æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿åé›†ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ")
    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        notify_slack("failure", f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")


if __name__ == "__main__":
    main()
