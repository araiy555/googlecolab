#!/usr/bin/env python3
"""
æ ªæ¢5å¹´åˆ†ãƒ‡ãƒ¼ã‚¿ã‚’éŠ˜æŸ„åˆ¥ã«æ•´ç†ã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ ï¼ˆãƒ¢ãƒ¼ãƒ‰å¯¾å¿œç‰ˆï¼‰
- full mode: 5å¹´åˆ†å…¨ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†
- incremental mode: ç›´è¿‘2ãƒ¶æœˆã®ã¿å‡¦ç†ã—ã¦æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ 
"""

import json
import boto3
import os
from datetime import datetime, timedelta
from collections import defaultdict, Counter
import logging
import re
from typing import Dict, List, Any, Optional, Tuple
import time

class StockBasedDataOrganizer:
    def __init__(self, mode='incremental'):
        """
        éŠ˜æŸ„åˆ¥ãƒ‡ãƒ¼ã‚¿æ•´ç†ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        
        Args:
            mode: 'full' (5å¹´åˆ†å…¨ã¦) ã¾ãŸã¯ 'incremental' (ç›´è¿‘2ãƒ¶æœˆã®ã¿)
        """
        self.mode = mode
        
        # AWSèªè¨¼æƒ…å ±ã‚’ç›´æ¥è¨­å®š
        import os
        aws_access_key = os.getenv('AWS_ACCESS_KEY_ID')
        aws_secret_key = os.getenv('AWS_SECRET_ACCESS_KEY')

        if aws_access_key and aws_secret_key:
            self.s3 = boto3.client(
                's3',
                aws_access_key_id=aws_access_key,
                aws_secret_access_key=aws_secret_key,
                region_name="ap-northeast-1"
            )
        else:
            self.s3 = boto3.client('s3', region_name="ap-northeast-1")
        self.bucket_name = "m-s3storage"

        # ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ‘ã‚¹ï¼ˆæ­£ç¢ºãªãƒ‘ã‚¹ï¼‰
        self.source_prefix = "japan-stocks-5years-chart/monthly-disclosures/"

        # å‡ºåŠ›ãƒ‘ã‚¹
        self.output_prefix = "japan-stocks-5years-chart/stock-based-disclosures/"

        # ãƒ­ã‚°è¨­å®š
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)

        # çµ±è¨ˆ
        self.stats = {
            'mode': mode,
            'total_disclosures': 0,
            'new_disclosures': 0,
            'duplicate_disclosures': 0,
            'unique_stocks': 0,
            'processed_months': 0,
            'created_files': 0,
            'updated_files': 0,
            'errors': 0,
            'start_time': None
        }

    def get_target_months(self) -> List[Tuple[int, int]]:
        """å¯¾è±¡æœˆã‚’å–å¾—ï¼ˆincrementalãƒ¢ãƒ¼ãƒ‰ç”¨ï¼‰"""
        today = datetime.now()
        current_year = today.year
        current_month = today.month

        months = []

        # å‰æœˆ
        if current_month == 1:
            prev_year = current_year - 1
            prev_month = 12
        else:
            prev_year = current_year
            prev_month = current_month - 1

        months.append((prev_year, prev_month))
        months.append((current_year, current_month))

        return months

    def get_monthly_files_list(self) -> List[str]:
        """S3ã‹ã‚‰æœˆæ¬¡ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã‚’å–å¾—"""
        try:
            paginator = self.s3.get_paginator('list_objects_v2')
            pages = paginator.paginate(Bucket=self.bucket_name, Prefix=self.source_prefix)

            monthly_files = []
            for page in pages:
                if 'Contents' in page:
                    for obj in page['Contents']:
                        key = obj['Key']
                        # æœˆæ¬¡ãƒ•ã‚¡ã‚¤ãƒ«ã®æ­£ç¢ºãªå½¢å¼: YYYY-MM.json
                        if key.endswith('.json') and re.search(r'/\d{4}-\d{2}\.json$', key):
                            monthly_files.append(key)

            monthly_files.sort()
            return monthly_files
            
        except Exception as e:
            self.logger.error(f"S3ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return []

    def load_existing_stock_data(self, stock_code: str) -> Optional[Dict]:
        """æ—¢å­˜ã®éŠ˜æŸ„ãƒ‡ãƒ¼ã‚¿ã‚’S3ã‹ã‚‰èª­ã¿è¾¼ã¿ï¼ˆincrementalãƒ¢ãƒ¼ãƒ‰ç”¨ï¼‰"""
        try:
            key = f"{self.output_prefix}{stock_code}.json"
            response = self.s3.get_object(Bucket=self.bucket_name, Key=key)
            data = json.loads(response['Body'].read().decode('utf-8'))
            return data
        except self.s3.exceptions.NoSuchKey:
            return None
        except Exception as e:
            self.logger.warning(f"æ—¢å­˜ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ ({stock_code}): {e}")
            return None

    def create_disclosure_key(self, disclosure: Dict) -> str:
        """é–‹ç¤ºãƒ‡ãƒ¼ã‚¿ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚­ãƒ¼ã‚’ç”Ÿæˆï¼ˆé‡è¤‡åˆ¤å®šç”¨ï¼‰"""
        stock_code = disclosure.get('stock_code', '')
        date = disclosure.get('date_normalized') or disclosure.get('date', '')
        title = disclosure.get('title', '')
        return f"{stock_code}_{date}_{title}"

    def merge_disclosures(self, existing: List[Dict], new: List[Dict]) -> List[Dict]:
        """æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¨æ–°è¦ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒ¼ã‚¸ï¼ˆé‡è¤‡æ’é™¤ï¼‰"""
        # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®ã‚­ãƒ¼ã‚»ãƒƒãƒˆä½œæˆ
        existing_keys = {self.create_disclosure_key(d) for d in existing}
        
        merged = existing.copy()
        new_count = 0
        duplicate_count = 0
        
        for disclosure in new:
            key = self.create_disclosure_key(disclosure)
            if key not in existing_keys:
                merged.append(disclosure)
                existing_keys.add(key)
                new_count += 1
            else:
                duplicate_count += 1
        
        self.stats['new_disclosures'] += new_count
        self.stats['duplicate_disclosures'] += duplicate_count
        
        return merged

    def load_all_monthly_data(self) -> Dict[str, List[Dict]]:
        """å…¨æœˆæ¬¡ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€éŠ˜æŸ„åˆ¥ã«æ•´ç†"""
        stock_data = defaultdict(list)

        try:
            self.logger.info(f"ğŸ“¦ {self.mode.upper()} MODE: ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–‹å§‹")
            
            # ãƒ¢ãƒ¼ãƒ‰ã«å¿œã˜ã¦ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã‚’æ±ºå®š
            if self.mode == 'full':
                # å…¨ãƒ•ã‚¡ã‚¤ãƒ«å–å¾—
                monthly_files = self.get_monthly_files_list()
                self.logger.info(f"5å¹´åˆ†å…¨ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†: {len(monthly_files)}ãƒ•ã‚¡ã‚¤ãƒ«")
            else:
                # ç›´è¿‘2ãƒ¶æœˆã®ã¿
                target_months = self.get_target_months()
                monthly_files = []
                for year, month in target_months:
                    file_key = f"{self.source_prefix}{year}-{month:02d}.json"
                    monthly_files.append(file_key)
                self.logger.info(f"ç›´è¿‘2ãƒ¶æœˆã®ã¿å‡¦ç†: {target_months[0][0]}å¹´{target_months[0][1]}æœˆ ã¨ {target_months[1][0]}å¹´{target_months[1][1]}æœˆ")

            if not monthly_files:
                self.logger.error(f"æœˆæ¬¡ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return {}

            # å„æœˆæ¬¡ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
            for i, file_key in enumerate(monthly_files, 1):
                try:
                    self.logger.info(f"[{i}/{len(monthly_files)}] å‡¦ç†ä¸­: {os.path.basename(file_key)}")

                    # S3ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«å–å¾—
                    response = self.s3.get_object(Bucket=self.bucket_name, Key=file_key)
                    file_content = response['Body'].read().decode('utf-8')
                    data = json.loads(file_content)

                    # ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã®ç¢ºèªã¨é–‹ç¤ºæƒ…å ±ã®æŠ½å‡º
                    disclosures = []

                    if isinstance(data, dict) and 'disclosures' in data:
                        disclosures = data['disclosures']
                    elif isinstance(data, list):
                        disclosures = data
                    elif isinstance(data, dict) and 'stock_code' in data:
                        disclosures = [data]

                    # éŠ˜æŸ„åˆ¥ã«åˆ†é¡
                    processed_count = 0
                    filtered_count = 0
                    for disclosure in disclosures:
                        if not isinstance(disclosure, dict):
                            continue

                        stock_code = disclosure.get('stock_code')
                        if stock_code and re.match(r'^[\dA-Za-z]{4}$', str(stock_code).strip()):
                            stock_code = str(stock_code).strip()

                            # ğŸ”§ ä¿®æ­£2: ã‚´ãƒŸãƒ¬ã‚³ãƒ¼ãƒ‰é™¤å¤–
                            title = disclosure.get('title', '')
                            cn = disclosure.get('company_name', '')
                            if any(kw in title for kw in ['æ¬¡ã¸', 'å‰ã¸', 'ï¼Â»', 'Â«ï¼œ']):
                                filtered_count += 1
                                continue
                            if any(kw in cn for kw in ['æ¬¡ã¸', 'å‰ã¸', 'ï¼Â»', 'Â«ï¼œ']):
                                filtered_count += 1
                                continue
                            if cn == 'æŠ½å‡ºä¸­' and (len(title) > 100 or '\n' in title):
                                filtered_count += 1
                                continue

                            enhanced_disclosure = self.enhance_disclosure_data(disclosure)
                            stock_data[stock_code].append(enhanced_disclosure)
                            processed_count += 1
                            self.stats['total_disclosures'] += 1

                    skip_info = f" (é™¤å¤–: {filtered_count}ä»¶)" if filtered_count > 0 else ""
                    self.logger.info(f"  âœ“ å‡¦ç†å®Œäº†: {processed_count:,}ä»¶{skip_info}")
                    self.stats['processed_months'] += 1

                except Exception as e:
                    self.logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {os.path.basename(file_key)} - {e}")
                    self.stats['errors'] += 1
                    continue

            self.stats['unique_stocks'] = len(stock_data)
            self.logger.info(f"\nãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†:")
            self.logger.info(f"  - å‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {self.stats['processed_months']}")
            self.logger.info(f"  - èª­è¾¼é–‹ç¤ºä»¶æ•°: {self.stats['total_disclosures']:,}ä»¶")
            self.logger.info(f"  - ãƒ¦ãƒ‹ãƒ¼ã‚¯éŠ˜æŸ„æ•°: {self.stats['unique_stocks']:,}éŠ˜æŸ„")

            return dict(stock_data)

        except Exception as e:
            self.logger.error(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å…¨ä½“ã‚¨ãƒ©ãƒ¼: {e}")
            return {}

    def enhance_disclosure_data(self, disclosure: Dict) -> Dict:
        """é–‹ç¤ºãƒ‡ãƒ¼ã‚¿ã®æ‹¡å¼µãƒ»å¼·åŒ– - æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®ã¿ä½¿ç”¨"""
        enhanced = disclosure.copy()

        # åŸºæœ¬æƒ…å ±ã®æ­£è¦åŒ–
        stock_code = enhanced.get('stock_code', '').strip()
        enhanced['stock_code'] = stock_code

        # æ—¢å­˜ã®ä¼šç¤¾åã‚’ãã®ã¾ã¾ä½¿ç”¨
        company_name = enhanced.get('company_name', '').strip()
        if company_name and company_name not in ['æŠ½å‡ºä¸­', 'ä¸æ˜', '']:
            enhanced['company_name_cleaned'] = self.normalize_company_name(company_name)
        else:
            enhanced['company_name_cleaned'] = f"éŠ˜æŸ„{stock_code}"

        # æ—¥ä»˜ã®æ­£è¦åŒ–
        date_str = enhanced.get('date', '')
        if date_str:
            enhanced['date_normalized'] = self.normalize_date(date_str)
            enhanced['year'] = int(enhanced['date_normalized'][:4]) if enhanced['date_normalized'] else None
            enhanced['month'] = int(enhanced['date_normalized'][5:7]) if enhanced['date_normalized'] else None
            enhanced['quarter'] = self.get_quarter(enhanced['month']) if enhanced['month'] else None

        # ã‚«ãƒ†ã‚´ãƒªã®è©³ç´°åˆ†é¡
        enhanced['category_detailed'] = self.detailed_categorization(
            enhanced.get('title', ''),
            enhanced.get('company_name', '')
        )

        # é‡è¦åº¦ã‚¹ã‚³ã‚¢ã®è¨ˆç®—
        enhanced['importance_score'] = self.calculate_importance_score(enhanced)

        # é–‹ç¤ºã‚¿ã‚¤ãƒ—ã®åˆ†é¡
        enhanced['disclosure_type'] = self.classify_disclosure_type(enhanced)

        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—è¿½åŠ 
        enhanced['processed_at'] = datetime.now().isoformat()

        return enhanced

    def normalize_company_name(self, company_name: str) -> str:
        """ä¼šç¤¾åã®æ­£è¦åŒ–"""
        patterns_to_remove = [
            r'\s*\d{2}/\d{2}/\d{2}\s*\d{2}:\d{2}$',
            r'\s*ç¬¬\d+æœŸ.*$',
            r'\s*Notice.*$',
            r'\s*\d+$',
        ]

        normalized = company_name
        for pattern in patterns_to_remove:
            normalized = re.sub(pattern, '', normalized)

        normalized = normalized.replace('(æ ª)', '').replace('ãˆ±', '')
        return normalized.strip()

    def normalize_date(self, date_str: str) -> str:
        """æ—¥ä»˜ã®æ­£è¦åŒ–"""
        if not date_str:
            return ''

        if re.match(r'^\d{4}-\d{2}-\d{2}$', date_str):
            return date_str

        patterns = [
            (r'^(\d{4})/(\d{1,2})/(\d{1,2})$', lambda m: f"{m.group(1)}-{int(m.group(2)):02d}-{int(m.group(3)):02d}"),
            (r'^(\d{1,2})/(\d{1,2})/(\d{4})$', lambda m: f"{m.group(3)}-{int(m.group(1)):02d}-{int(m.group(2)):02d}"),
            (r'^(\d{4})(\d{2})(\d{2})$', lambda m: f"{m.group(1)}-{m.group(2)}-{m.group(3)}"),
        ]

        for pattern, formatter in patterns:
            match = re.match(pattern, date_str)
            if match:
                try:
                    return formatter(match)
                except ValueError:
                    continue

        return date_str

    def get_quarter(self, month: int) -> int:
        """æœˆã‹ã‚‰å››åŠæœŸã‚’å–å¾—"""
        if month in [1, 2, 3]:
            return 1
        elif month in [4, 5, 6]:
            return 2
        elif month in [7, 8, 9]:
            return 3
        else:
            return 4

    def detailed_categorization(self, title: str, company_name: str = '') -> str:
        """è©³ç´°ã‚«ãƒ†ã‚´ãƒªåˆ†é¡"""
        text = f"{title} {company_name}".lower()

        detailed_categories = {
            'æ±ºç®—çŸ­ä¿¡': ['æ±ºç®—çŸ­ä¿¡', 'å››åŠæœŸæ±ºç®—çŸ­ä¿¡'],
            'æ±ºç®—èª¬æ˜ä¼š': ['æ±ºç®—èª¬æ˜ä¼š', 'æ±ºç®—briefing', 'æ¥­ç¸¾èª¬æ˜ä¼š'],
            'æ¥­ç¸¾äºˆæƒ³ä¿®æ­£': ['æ¥­ç¸¾äºˆæƒ³', 'æ¥­ç¸¾è¦‹é€šã—ä¿®æ­£', 'æ¥­ç¸¾ä¿®æ­£'],
            'é…å½“é‡‘': ['é…å½“é‡‘', 'æœŸæœ«é…å½“', 'ä¸­é–“é…å½“', 'ç‰¹åˆ¥é…å½“'],
            'æ ªä¸»å„ªå¾…': ['æ ªä¸»å„ªå¾…', 'å„ªå¾…åˆ¶åº¦'],
            'è‡ªå·±æ ªå¼å–å¾—': ['è‡ªå·±æ ªå¼å–å¾—', 'è‡ªç¤¾æ ªè²·ã„'],
            'æ ªå¼åˆ†å‰²': ['æ ªå¼åˆ†å‰²', 'æ ªå¼ä½µåˆ'],
            'æ–°æ ªäºˆç´„æ¨©': ['æ–°æ ªäºˆç´„æ¨©', 'ã‚¹ãƒˆãƒƒã‚¯ã‚ªãƒ—ã‚·ãƒ§ãƒ³'],
            'ç¬¬ä¸‰è€…å‰²å½“å¢—è³‡': ['ç¬¬ä¸‰è€…å‰²å½“', 'ç¬¬ä¸‰è€…å‰²å½“å¢—è³‡'],
            'ä»£è¡¨å–ç· å½¹': ['ä»£è¡¨å–ç· å½¹', 'CEO', 'ç¤¾é•·'],
            'å½¹å“¡äººäº‹': ['å–ç· å½¹', 'ç›£æŸ»å½¹', 'åŸ·è¡Œå½¹å“¡'],
            'M&Aè²·å': ['è²·å', 'æ ªå¼å–å¾—', 'å­ä¼šç¤¾åŒ–'],
            'æ¥­å‹™ææº': ['æ¥­å‹™ææº', 'è³‡æœ¬ææº'],
            'æ–°è¦äº‹æ¥­': ['æ–°è¦äº‹æ¥­', 'äº‹æ¥­å‚å…¥'],
            'æ–°è£½å“ç™ºè¡¨': ['æ–°è£½å“', 'æ–°å•†å“'],
            'è¨­å‚™æŠ•è³‡': ['è¨­å‚™æŠ•è³‡', 'å·¥å ´å»ºè¨­'],
            'é©æ™‚é–‹ç¤ºè¨‚æ­£': ['è¨‚æ­£', 'ä¿®æ­£', 'å–æ¶ˆ'],
            'æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸': ['æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸', 'å››åŠæœŸå ±å‘Šæ›¸'],
            'å®šæ™‚æ ªä¸»ç·ä¼š': ['å®šæ™‚æ ªä¸»ç·ä¼š'],
            'è‡¨æ™‚æ ªä¸»ç·ä¼š': ['è‡¨æ™‚æ ªä¸»ç·ä¼š'],
        }

        for category, keywords in detailed_categories.items():
            if any(keyword in text for keyword in keywords):
                return category

        return 'ãã®ä»–'

    def calculate_importance_score(self, disclosure: Dict) -> float:
        """é‡è¦åº¦ã‚¹ã‚³ã‚¢ã®è¨ˆç®—ï¼ˆ0.0-1.0ï¼‰"""
        score = 0.0
        title = disclosure.get('title', '').lower()
        category = disclosure.get('category_detailed', '')

        high_importance_categories = [
            'æ±ºç®—çŸ­ä¿¡', 'æ¥­ç¸¾äºˆæƒ³ä¿®æ­£', 'ä»£è¡¨å–ç· å½¹', 'M&Aè²·å',
            'ç¬¬ä¸‰è€…å‰²å½“å¢—è³‡'
        ]

        medium_importance_categories = [
            'æ±ºç®—èª¬æ˜ä¼š', 'é…å½“é‡‘', 'æ ªå¼åˆ†å‰²', 'è‡ªå·±æ ªå¼å–å¾—', 'æ–°è¦äº‹æ¥­',
            'æ¥­å‹™ææº', 'è¨­å‚™æŠ•è³‡'
        ]

        if category in high_importance_categories:
            score += 0.5
        elif category in medium_importance_categories:
            score += 0.3
        else:
            score += 0.1

        high_impact_keywords = [
            'æ¥­ç¸¾äºˆæƒ³ä¿®æ­£', 'èµ¤å­—', 'é»’å­—è»¢æ›', 'å¢—é…', 'æ¸›é…', 'ç„¡é…',
            'è²·å', 'åˆä½µ'
        ]

        for keyword in high_impact_keywords:
            if keyword in title:
                score += 0.2
                break

        if re.search(r'\d+å„„å††|\d+ç™¾ä¸‡å††|\d+åƒä¸‡å††', title):
            score += 0.1

        if len(disclosure.get('title', '')) > 50:
            score += 0.1

        return min(score, 1.0)

    def classify_disclosure_type(self, disclosure: Dict) -> str:
        """é–‹ç¤ºã‚¿ã‚¤ãƒ—ã®åˆ†é¡"""
        category = disclosure.get('category_detailed', '')
        title = disclosure.get('title', '').lower()

        if any(keyword in title for keyword in ['æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸', 'å››åŠæœŸå ±å‘Šæ›¸', 'æ±ºç®—çŸ­ä¿¡']):
            return 'æ³•å®šé–‹ç¤º'
        elif any(keyword in category for keyword in ['æ¥­ç¸¾äºˆæƒ³', 'M&A', 'äººäº‹', 'é…å½“']):
            return 'é©æ™‚é–‹ç¤º'
        elif any(keyword in title for keyword in ['èª¬æ˜è³‡æ–™', 'ãƒ—ãƒ¬ã‚¼ãƒ³', 'èª¬æ˜ä¼š']):
            return 'IRè³‡æ–™'
        else:
            return 'ãã®ä»–'

    def create_stock_summary(self, stock_code: str, disclosures: List[Dict]) -> Dict:
        """éŠ˜æŸ„ã‚µãƒãƒªãƒ¼ã®ä½œæˆ - é–‹ç¤ºãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡º"""
        if not disclosures:
            return {}

        total_disclosures = len(disclosures)
        date_range = self.get_date_range(disclosures)

        category_stats = Counter(d.get('category_detailed', 'ãã®ä»–') for d in disclosures)
        yearly_stats = Counter(d.get('year') for d in disclosures if d.get('year'))

        quarterly_stats = defaultdict(lambda: defaultdict(int))
        for d in disclosures:
            year = d.get('year')
            quarter = d.get('quarter')
            if year and quarter:
                quarterly_stats[year][f"Q{quarter}"] += 1

        importance_distribution = {
            'high': len([d for d in disclosures if d.get('importance_score', 0) >= 0.7]),
            'medium': len([d for d in disclosures if 0.4 <= d.get('importance_score', 0) < 0.7]),
            'low': len([d for d in disclosures if d.get('importance_score', 0) < 0.4])
        }

        company_names = [d.get('company_name_cleaned') or d.get('company_name', '')
                        for d in disclosures
                        if d.get('company_name_cleaned') or d.get('company_name', '')]
        company_names = [name for name in company_names if name and name != f"éŠ˜æŸ„{stock_code}"]

        if company_names:
            name_counter = Counter(company_names)
            most_common_name = name_counter.most_common(1)[0][0]
        else:
            most_common_name = f"éŠ˜æŸ„{stock_code}"

        company_info = {
            'stock_code': stock_code,
            'company_name': most_common_name,
            'company_name_variations': list(set(company_names[:5])),
            'data_source': 'é–‹ç¤ºãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æŠ½å‡º'
        }

        major_disclosures = sorted(
            [d for d in disclosures if d.get('importance_score', 0) >= 0.6],
            key=lambda x: (x.get('date_normalized', ''), x.get('importance_score', 0)),
            reverse=True
        )[:20]

        # æœŸé–“ã‚’è¨ˆç®—
        if date_range.get('start_date') and date_range.get('end_date'):
            try:
                start = datetime.strptime(date_range['start_date'], '%Y-%m-%d')
                end = datetime.strptime(date_range['end_date'], '%Y-%m-%d')
                period_years = (end - start).days / 365.25
            except:
                period_years = len(yearly_stats)
        else:
            period_years = len(yearly_stats)

        return {
            'company_info': company_info,
            'summary_stats': {
                'total_disclosures': total_disclosures,
                'date_range': date_range,
                'analysis_period_years': round(period_years, 1),
                'average_disclosures_per_year': round(total_disclosures / max(period_years, 1), 1)
            },
            'category_distribution': dict(category_stats.most_common()),
            'yearly_trend': dict(yearly_stats),
            'quarterly_trend': dict(quarterly_stats),
            'importance_distribution': importance_distribution,
            'major_disclosures': major_disclosures
        }

    def get_date_range(self, disclosures: List[Dict]) -> Dict[str, str]:
        """æ—¥ä»˜ç¯„å›²ã®å–å¾—"""
        dates = [d.get('date_normalized') for d in disclosures if d.get('date_normalized')]
        dates = [d for d in dates if d and re.match(r'^\d{4}-\d{2}-\d{2}$', d)]

        if dates:
            return {
                'start_date': min(dates),
                'end_date': max(dates)
            }
        return {'start_date': '', 'end_date': ''}

    def save_stock_data_to_s3(self, stock_code: str, disclosures: List[Dict], is_update: bool = False) -> bool:
        """éŠ˜æŸ„åˆ¥ãƒ‡ãƒ¼ã‚¿ã‚’S3ã«ä¿å­˜"""
        try:
            sorted_disclosures = sorted(disclosures, key=lambda x: x.get('date_normalized', ''))
            summary = self.create_stock_summary(stock_code, sorted_disclosures)

            stock_data = {
                'metadata': {
                    'stock_code': stock_code,
                    'created_at': datetime.now().isoformat(),
                    'data_version': '1.0',
                    'source': 'æ ªæ¢5å¹´åˆ†ãƒ‡ãƒ¼ã‚¿',
                    'last_updated': datetime.now().isoformat()
                },
                'summary': summary,
                'disclosures': sorted_disclosures
            }

            key = f"{self.output_prefix}{stock_code}.json"
            json_data = json.dumps(stock_data, ensure_ascii=False, indent=2)

            self.s3.put_object(
                Bucket=self.bucket_name,
                Key=key,
                Body=json_data.encode('utf-8'),
                ContentType='application/json',
                ServerSideEncryption='AES256'
            )

            file_size_kb = len(json_data) / 1024
            action = "æ›´æ–°" if is_update else "ä½œæˆ"
            self.logger.info(f"{action}å®Œäº†: {stock_code} ({len(sorted_disclosures)}ä»¶, {file_size_kb:.1f}KB)")
            
            if is_update:
                self.stats['updated_files'] += 1
            else:
                self.stats['created_files'] += 1

            return True

        except Exception as e:
            self.logger.error(f"ä¿å­˜ã‚¨ãƒ©ãƒ¼ ({stock_code}): {e}")
            self.stats['errors'] += 1
            return False

    def create_master_index(self, stock_data: Dict[str, List[Dict]]):
        """éŠ˜æŸ„ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ"""
        try:
            index_data = {
                'metadata': {
                    'created_at': datetime.now().isoformat(),
                    'total_stocks': len(stock_data),
                    'total_disclosures': sum(len(disclosures) for disclosures in stock_data.values()),
                    'data_source': 'æ ªæ¢5å¹´åˆ†ãƒ‡ãƒ¼ã‚¿'
                },
                'stocks': {}
            }

            for stock_code, disclosures in stock_data.items():
                if disclosures:
                    latest = max(disclosures, key=lambda x: x.get('date_normalized', ''), default={})
                    date_range = self.get_date_range(disclosures)

                    index_data['stocks'][stock_code] = {
                        'company_name': latest.get('company_name_cleaned') or latest.get('company_name', 'ä¸æ˜'),
                        'total_disclosures': len(disclosures),
                        'date_range': date_range,
                        'file_path': f"{self.output_prefix}{stock_code}.json"
                    }

            index_key = f"{self.output_prefix}index.json"
            self.s3.put_object(
                Bucket=self.bucket_name,
                Key=index_key,
                Body=json.dumps(index_data, ensure_ascii=False, indent=2).encode('utf-8'),
                ContentType='application/json'
            )

            self.logger.info(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆå®Œäº†: {index_key}")

        except Exception as e:
            self.logger.error(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")

    def process_stocks_in_batches(self, new_stock_data: Dict[str, List[Dict]], batch_size: int = 50):
        """éŠ˜æŸ„ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒƒãƒå‡¦ç†ï¼ˆãƒ¢ãƒ¼ãƒ‰åˆ¥å‡¦ç†ï¼‰"""
        stock_codes = list(new_stock_data.keys())
        total_stocks = len(stock_codes)

        self.logger.info(f"\néŠ˜æŸ„ãƒ‡ãƒ¼ã‚¿å‡¦ç†é–‹å§‹: {total_stocks:,}éŠ˜æŸ„ (mode={self.mode})")

        for i, stock_code in enumerate(stock_codes, 1):
            try:
                if self.mode == 'incremental':
                    # å·®åˆ†æ›´æ–°ãƒ¢ãƒ¼ãƒ‰ï¼šæ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§ãƒãƒ¼ã‚¸
                    existing_data = self.load_existing_stock_data(stock_code)
                    
                    if existing_data and 'disclosures' in existing_data:
                        # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚ã‚Šï¼šãƒãƒ¼ã‚¸
                        existing_disclosures = existing_data['disclosures']
                        new_disclosures = new_stock_data[stock_code]
                        merged_disclosures = self.merge_disclosures(existing_disclosures, new_disclosures)
                        
                        if len(merged_disclosures) > len(existing_disclosures):
                            # æ–°è¦ãƒ‡ãƒ¼ã‚¿ãŒã‚ã£ãŸå ´åˆã®ã¿ä¿å­˜
                            self.logger.info(f"[{i}/{total_stocks}] æ›´æ–°: {stock_code} (+{len(merged_disclosures)-len(existing_disclosures)}ä»¶)")
                            self.save_stock_data_to_s3(stock_code, merged_disclosures, is_update=True)
                        else:
                            # æ–°è¦ãƒ‡ãƒ¼ã‚¿ãªã—
                            self.logger.info(f"[{i}/{total_stocks}] ã‚¹ã‚­ãƒƒãƒ—: {stock_code} (å¤‰æ›´ãªã—)")
                    else:
                        # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ãªã—ï¼šæ–°è¦ä½œæˆ
                        self.logger.info(f"[{i}/{total_stocks}] æ–°è¦ä½œæˆ: {stock_code}")
                        self.save_stock_data_to_s3(stock_code, new_stock_data[stock_code], is_update=False)
                
                else:
                    # ãƒ•ãƒ«ãƒ¢ãƒ¼ãƒ‰ï¼šå¸¸ã«ä¸Šæ›¸ã
                    self.logger.info(f"[{i}/{total_stocks}] å‡¦ç†: {stock_code}")
                    self.save_stock_data_to_s3(stock_code, new_stock_data[stock_code], is_update=False)

                # é€²æ—è¡¨ç¤º
                if i % 100 == 0:
                    elapsed = time.time() - self.stats['start_time']
                    progress = i / total_stocks * 100
                    self.logger.info(f"  é€²æ—: {progress:.1f}% ({i}/{total_stocks}) - çµŒéæ™‚é–“: {elapsed/60:.1f}åˆ†")

                time.sleep(0.1)

            except Exception as e:
                self.logger.error(f"å‡¦ç†ã‚¨ãƒ©ãƒ¼ ({stock_code}): {e}")
                self.stats['errors'] += 1

        self.logger.info("éŠ˜æŸ„ãƒ‡ãƒ¼ã‚¿å‡¦ç†å®Œäº†")

    def run_stock_based_organization(self):
        """éŠ˜æŸ„åˆ¥ãƒ‡ãƒ¼ã‚¿æ•´ç†ã®å®Ÿè¡Œ"""
        print("=" * 80)
        print(f"æ ªæ¢ãƒ‡ãƒ¼ã‚¿éŠ˜æŸ„åˆ¥æ•´ç†ã‚·ã‚¹ãƒ†ãƒ  (mode={self.mode})")
        print("=" * 80)

        self.stats['start_time'] = time.time()

        try:
            # Step 1: ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
            self.logger.info("Step 1: ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–‹å§‹")
            stock_data = self.load_all_monthly_data()

            if not stock_data:
                self.logger.error("ãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸ")
                return False

            # Step 2: çµ±è¨ˆè¡¨ç¤º
            if self.mode == 'full':
                self.logger.info("Step 2: ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ")
                self.display_statistics(stock_data)

            # Step 3: éŠ˜æŸ„åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ/æ›´æ–°
            self.logger.info("Step 3: éŠ˜æŸ„åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†é–‹å§‹")
            self.process_stocks_in_batches(stock_data)

            # Step 4: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆï¼ˆfullãƒ¢ãƒ¼ãƒ‰æ™‚ã®ã¿ï¼‰
            if self.mode == 'full':
                self.logger.info("Step 4: ãƒã‚¹ã‚¿ãƒ¼ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ")
                self.create_master_index(stock_data)

            # Step 5: å®Œäº†ãƒ¬ãƒãƒ¼ãƒˆ
            self.display_completion_report()

            return True

        except Exception as e:
            self.logger.error(f"å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
            return False

    def display_statistics(self, stock_data: Dict[str, List[Dict]]):
        """çµ±è¨ˆæƒ…å ±ã®è¡¨ç¤º"""
        print("\n" + "=" * 60)
        print("ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ")
        print("=" * 60)

        total_stocks = len(stock_data)
        total_disclosures = sum(len(disclosures) for disclosures in stock_data.values())

        print(f"å¯¾è±¡éŠ˜æŸ„æ•°: {total_stocks:,}")
        print(f"ç·é–‹ç¤ºä»¶æ•°: {total_disclosures:,}")
        print(f"éŠ˜æŸ„å¹³å‡é–‹ç¤ºä»¶æ•°: {total_disclosures / max(total_stocks, 1):.1f}")

        # ä¸Šä½éŠ˜æŸ„ï¼ˆé–‹ç¤ºä»¶æ•°é †ï¼‰
        top_stocks = sorted(
            [(code, len(disclosures)) for code, disclosures in stock_data.items()],
            key=lambda x: x[1],
            reverse=True
        )[:10]

        print("\nã€é–‹ç¤ºä»¶æ•°ä¸Šä½10éŠ˜æŸ„ã€‘")
        for i, (stock_code, count) in enumerate(top_stocks, 1):
            company_name = "ä¸æ˜"
            if stock_data[stock_code]:
                latest = stock_data[stock_code][0]
                company_name = latest.get('company_name_cleaned') or latest.get('company_name', 'ä¸æ˜')
            print(f"  {i:2d}. {stock_code} ({company_name}): {count:,}ä»¶")

    def display_completion_report(self):
        """å®Œäº†ãƒ¬ãƒãƒ¼ãƒˆã®è¡¨ç¤º"""
        elapsed_time = time.time() - self.stats['start_time']

        print("\n" + "=" * 80)
        print("éŠ˜æŸ„åˆ¥ãƒ‡ãƒ¼ã‚¿æ•´ç†å®Œäº†ãƒ¬ãƒãƒ¼ãƒˆ")
        print("=" * 80)
        print(f"å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰: {self.mode}")
        print(f"å‡¦ç†æ™‚é–“: {elapsed_time/60:.1f}åˆ†")
        print(f"å‡¦ç†æœˆæ•°: {self.stats['processed_months']}")
        print(f"å¯¾è±¡éŠ˜æŸ„æ•°: {self.stats['unique_stocks']:,}")
        print(f"èª­è¾¼é–‹ç¤ºä»¶æ•°: {self.stats['total_disclosures']:,}")
        
        if self.mode == 'incremental':
            print(f"æ–°è¦é–‹ç¤ºä»¶æ•°: {self.stats['new_disclosures']:,}")
            print(f"é‡è¤‡é™¤å¤–ä»¶æ•°: {self.stats['duplicate_disclosures']:,}")
            print(f"æ›´æ–°ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {self.stats['updated_files']:,}")
            print(f"æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {self.stats['created_files']:,}")
        else:
            print(f"ä½œæˆãƒ•ã‚¡ã‚¤ãƒ«æ•°: {self.stats['created_files']:,}")
        
        print(f"ã‚¨ãƒ©ãƒ¼ä»¶æ•°: {self.stats['errors']}")

        if (self.stats['created_files'] + self.stats['updated_files']) > 0:
            total_files = self.stats['created_files'] + self.stats['updated_files']
            print(f"å¹³å‡å‡¦ç†æ™‚é–“/éŠ˜æŸ„: {elapsed_time/total_files:.2f}ç§’")

        print(f"\nä¿å­˜å…ˆ: s3://{self.bucket_name}/{self.output_prefix}")
        print("=" * 80)

    def get_sample_stock_data(self, stock_code: str = "1301") -> Dict:
        """ã‚µãƒ³ãƒ—ãƒ«éŠ˜æŸ„ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰"""
        try:
            key = f"{self.output_prefix}{stock_code}.json"
            response = self.s3.get_object(Bucket=self.bucket_name, Key=key)
            data = json.loads(response['Body'].read().decode('utf-8'))
            return data
        except Exception as e:
            self.logger.error(f"ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼ ({stock_code}): {e}")
            return {}


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°ï¼ˆColabå¯¾å¿œç‰ˆï¼‰"""
    # ç’°å¢ƒå¤‰æ•°ã¾ãŸã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ãƒ¢ãƒ¼ãƒ‰ã‚’æ±ºå®š
    mode = os.getenv('STOCK_ORGANIZER_MODE', 'full')
    
    try:
        organizer = StockBasedDataOrganizer(mode=mode)
        success = organizer.run_stock_based_organization()

        if success:
            print(f"âœ… éŠ˜æŸ„åˆ¥ãƒ‡ãƒ¼ã‚¿æ•´ç†ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ (mode={mode})")

            # ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
            if mode == 'full':
                print(f"\nã‚µãƒ³ãƒ—ãƒ«: éŠ˜æŸ„1301ã®ãƒ‡ãƒ¼ã‚¿æ§‹é€ ")
                sample = organizer.get_sample_stock_data("1301")
                if sample and 'summary' in sample:
                    summary = sample['summary']
                    print(f"  ä¼šç¤¾å: {summary.get('company_info', {}).get('company_name', 'ä¸æ˜')}")
                    print(f"  é–‹ç¤ºä»¶æ•°: {summary.get('summary_stats', {}).get('total_disclosures', 0)}ä»¶")
                    print(f"  æœŸé–“: {summary.get('summary_stats', {}).get('date_range', {})}")
        else:
            print("âŒ å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")

    except Exception as e:
        print(f"âŒ å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()


# Colabç”¨ã®ç›´æ¥å®Ÿè¡Œé–¢æ•°
def run_debug_path():
    """S3ãƒ‘ã‚¹ç¢ºèªç”¨é–¢æ•°ï¼ˆColabç›´æ¥å®Ÿè¡Œç”¨ï¼‰"""
    organizer = StockBasedDataOrganizer()

    print("S3ãƒ‘ã‚¹ç¢ºèªãƒ¢ãƒ¼ãƒ‰")
    print(f"ãƒã‚±ãƒƒãƒˆ: {organizer.bucket_name}")
    print(f"å–å¾—ãƒ‘ã‚¹: {organizer.source_prefix}")
    print(f"å‡ºåŠ›ãƒ‘ã‚¹: {organizer.output_prefix}")

    try:
        paginator = organizer.s3.get_paginator('list_objects_v2')
        pages = paginator.paginate(Bucket=organizer.bucket_name, Prefix=organizer.source_prefix)

        print(f"\n{organizer.source_prefix} å†…ã®ãƒ•ã‚¡ã‚¤ãƒ«:")
        file_count = 0
        for page in pages:
            if 'Contents' in page:
                for obj in page['Contents']:
                    print(f"  {obj['Key']}")
                    file_count += 1
                    if file_count >= 10:
                        break
            if file_count >= 10:
                break

        if file_count == 0:
            print("  ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        else:
            print(f"  ç·è¨ˆ: {file_count}+ ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«")

    except Exception as e:
        print(f"S3ã‚¢ã‚¯ã‚»ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")


def run_full():
    """ãƒ•ãƒ«ãƒ¢ãƒ¼ãƒ‰å®Ÿè¡Œç”¨ï¼ˆColabç›´æ¥å®Ÿè¡Œç”¨ï¼‰"""
    os.environ['STOCK_ORGANIZER_MODE'] = 'full'
    organizer = StockBasedDataOrganizer(mode='full')
    success = organizer.run_stock_based_organization()
    
    if success:
        print("âœ… ãƒ•ãƒ«ãƒ¢ãƒ¼ãƒ‰å‡¦ç†ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ")
    else:
        print("âŒ å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")


def run_incremental():
    """å·®åˆ†æ›´æ–°ãƒ¢ãƒ¼ãƒ‰å®Ÿè¡Œç”¨ï¼ˆColabç›´æ¥å®Ÿè¡Œç”¨ï¼‰"""
    os.environ['STOCK_ORGANIZER_MODE'] = 'incremental'
    organizer = StockBasedDataOrganizer(mode='incremental')
    success = organizer.run_stock_based_organization()
    
    if success:
        print("âœ… å·®åˆ†æ›´æ–°ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ")
    else:
        print("âŒ å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")


def get_sample(stock_code="1301"):
    """ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿å–å¾—ç”¨é–¢æ•°ï¼ˆColabç›´æ¥å®Ÿè¡Œç”¨ï¼‰"""
    organizer = StockBasedDataOrganizer()

    print(f"éŠ˜æŸ„ {stock_code} ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿:")
    sample_data = organizer.get_sample_stock_data(stock_code)
    if sample_data:
        print(json.dumps(sample_data, ensure_ascii=False, indent=2)[:2000] + "...")
    else:
        print("ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")


if __name__ == "__main__":
    main()
